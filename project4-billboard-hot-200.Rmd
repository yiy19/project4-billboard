---
title: "Project 4 - Billboard Hot 200"
author: "Ye Dam Yi"
date: "3/17/2021"
output: html_document
---
```{r library}
library(tidyverse) 
library(skimr)
library(glue)
```

### Read in data
```{R load-data}
billboard <- read_csv("data/billboard.csv")
```

### Data inspection and manipulation
```{r inspect-data}
billboard <- billboard %>% 
  mutate(last = as.numeric(last),
         artisttitle = paste0(artist, "::", title))
skim(billboard)


```


Songs on the Billboard Hot 200 chart hit the 14th on the chart as the peak rank on average. They also stay on the chart for 137 weeks on average. However, the standard deviations are very large.

### What I learned so far
What I learned is that I need to ask myself this question: "What data do I have? And what are some interesting questions I can answer with the data that I can represent in a legible way?"

Following that question I could lead myself to think that given I have that the Billboard's top 200 songs, which is too many, I could narrow it down to a list of 10. One criterion for determining that list could be the longest. 

### Top 25 albums that have been on the chart for the longest
```{r top-10-albums}
billboard %>% 
  arrange(desc(week)) %>% 
  slice_head(n = 25) %>% 
  ggplot(aes(x = week, y = title)) +
  geom_bar(stat = "identity", fill = "sky blue") +
  labs(
    title = "25 Albums with the Longest Streak on Billboard Hot 200",
    x = "Weeks on Billboard Hot 200",
    y = "Albums"
  ) +
  theme_minimal()
```


This graph was looking strange with the Greatest Hits having more than 2000 weeks in a row. Data inspection told me that there was something wrong. It seemed that the albums with the same title "Greatest Hits" released by different artists were merged into one item on the graph. I needed to differentiate different artists' "Greatest Hits." In order to do that, I added a variable pasting the artist name and album title together. 


### Top 10 albums that have been on the chart for the longest
```{r top-10-albums-corrected, fig.height=5, fig.width=11.5}

billboard %>% 
  arrange(desc(week)) %>% 
  slice_head(n = 10) %>% 
  ggplot(aes(x = week, y = artisttitle)) +
  geom_bar(stat = "identity", fill = "sky blue") +
  labs(
    title = "Top 10 albums with the Longest Streak on Billboard Hot 200",
    x = "Weeks on Billboard Hot 200",
    y = "Artist::Album"
  ) +
  theme_minimal()

```


The plot with 25 albums looked really crowded, so I decided to include only the 10 albums.


### Artists with the most albums on the chart
```{r artists-most-albums, eval = FALSE}
billboard %>% 
  count(artist) %>% 
  arrange(desc(n)) %>% 
  slice_head(n = 10) %>% 
  ggplot(aes(x = n, y = artist)) +
  geom_bar(stat = "identity", fill = "sky blue") +
  labs(
    title = "Taylor Swift and Drake have the Most Albums on Billboard Hot 200",
    y = "Artists",
    x = "Number of Albums on Billboard Hot 200"
  )
  theme_minimal()  
```



### What I learned from this project
1. Ask questions
Ask myself what data I have, what interesting questions I could ask, and what questions would lead to legible data. 

2. Inspect data thoroughly
Check if there is any strange data on my plots. If there is, figure out why the data are that way. Make corrections as needed.

3. Make sure I'm using object names that exist
Initially I wrote out a csv file named billboard without creating an object called billboard. That resulted in an output file that I did not intend to have. I have to make sure that all the objects that I call in code are actually assigned by me. 

4. Make sure I am very intentional about what I do
Initially I identified html tags using SelectorGadget, but I kept getting duplicate html tags or not getting what data I wanted to get. So I used the "inspection" feature to really nail down the html tags I wanted to extract information from. I learned that sometimes certain rendering tags and information tags get merged together, making it crucial to parcel out one from the other to get the content only. 

5. Learn how to think like a data scientist
I still struggle with a lot of things, which stems from not having the mindset of a good data scientist. This will come with repetition and practice. 

